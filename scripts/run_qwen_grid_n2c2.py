#!/usr/bin/env python3
"""
Grid Search de Qwen para n2c2_developt
Optimiza chunks y overlaps usando qwen25_optimized
"""

import itertools
import subprocess
import sys
import os
from datetime import datetime

def main():
    chunk_targets = [20, 40, 60, 80, 100]
    overlaps = [10, 20, 30, 40, 50]

    # Crear directorio para resultados
    results_dir = f"qwen_grid_n2c2_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    os.makedirs(results_dir, exist_ok=True)

    combos = [(t, o) for t in chunk_targets for o in overlaps if o < t]
    print(f"ðŸš€ Grid Search Qwen para n2c2: {len(combos)} configuraciones")
    print(f"ðŸ“ Resultados en: {results_dir}")
    print(f"ðŸ“Š Dataset: n2c2_developt")
    print()

    successful_runs = 0
    failed_runs = 0

    for i, (t, o) in enumerate(combos, 1):
        print(f"ðŸ”„ [{i:2d}/{len(combos)}] chunk={t:3d}, overlap={o:3d}")
        
        out_file = f"results_qwen_n2c2_chunk{t}_ov{o}.jsonl"
        out_path = os.path.join(results_dir, out_file)
        
        cmd = [
            sys.executable,
            "scripts/llama_ner_multi_strategy.py",
            "--input_jsonl",
            "datasets/n2c2_developt_input.jsonl",
            "--benchmark_jsonl",
            "datasets/n2c2_developt.jsonl",
            "--out_pred",
            out_path,
            "--limit",
            "20",  # Usar 20 documentos para evaluaciÃ³n
            "--confidence_threshold",
            "0.3",
            "--strategies",
            "qwen25_optimized",
            "--s1_target",
            str(t),
            "--s1_overlap",
            str(o),
            "--s1_temp",
            "0.5",
        ]
        
        print(f"   ðŸ“ Ejecutando: {out_file}")
        try:
            result = subprocess.run(cmd, check=True, capture_output=True, text=True)
            print(f"   âœ… Ã‰xito: {out_file}")
            successful_runs += 1
            
            # Evaluar rendimiento inmediatamente
            evaluate_performance(out_path)
            
        except subprocess.CalledProcessError as e:
            print(f"   âŒ Error: {e}")
            failed_runs += 1
        
        print()
    
    # Resumen final
    print("=" * 60)
    print("ðŸ GRID SEARCH QWEN COMPLETADO")
    print("=" * 60)
    print(f"âœ… Ejecuciones exitosas: {successful_runs}")
    print(f"âŒ Ejecuciones fallidas: {failed_runs}")
    print(f"ðŸ“ Resultados guardados en: {results_dir}")
    print()
    print("ðŸ“Š Para analizar resultados:")
    print(f"   python scripts/analyze_grid.py {results_dir}")

def evaluate_performance(result_file):
    """EvalÃºa el rendimiento de una configuraciÃ³n especÃ­fica"""
    try:
        cmd = [
            sys.executable,
            "scripts/evaluate_ner_performance.py",
            "--predictions",
            result_file,
            "--reference",
            "datasets/n2c2_developt.jsonl"
        ]
        
        result = subprocess.run(cmd, check=True, capture_output=True, text=True)
        output = result.stdout
        
        # Extraer mÃ©tricas clave
        for line in output.split('\n'):
            if "PrecisiÃ³n:" in line:
                precision = line.split(":")[1].strip().split()[0]
                print(f"      ðŸ“Š P={precision}")
                break
                
    except subprocess.CalledProcessError:
        print(f"      âš ï¸  No se pudo evaluar rendimiento")

if __name__ == "__main__":
    main()
